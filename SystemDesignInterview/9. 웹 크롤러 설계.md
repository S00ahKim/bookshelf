# 웹 크롤러 설계

## 웹 크롤러
- 목적: 웹에 새로 올라오거나 갱신된 콘텐츠 찾기
- 방법: 몇 개 웹페이지에서 시작해서 그 링크를 따라 나가면서 새로운 콘텐츠를 수집
- 이용예 `복잡도는 규모와 기능에 따라 천차만별이므로, 확인 필요`
    * 검색 엔진 인덱싱 (구글)
    * 웹 아카이빙 (장기 보관용, 도서관)
    * 웹 마이닝 (유용한 지식 도출)
    * 웹 모니터링 (권익 침해 사례 확인)
- 좋은 크롤러가 만족시키는 속성
    * 규모 확장성 by parallelism
    * 안정성: 장애/악성코드에 대한 대응
    * 예절: 짧은 시간 동안 너무 많은 요청 X
    * 확장성: 다양한 형태의 콘텐츠 지원에 용이

## 설계 인터뷰 대응
1. 문제 이해 및 설계 범위 확정
    * 질문으로 용도/수집량/대강의 기능을 구한다.
    * 개력적 규모 추정
        - 매달의 처리량을 1초 단위로 수행해야 할 정도로 분해해서 QPS 구하기
        - 웹 페이지의 평균 크기를 추정(ex. 500kb)하고 처리량으로 월단위 필요 용량을 산정 
2. 개략적인 설계안 제시 및 동의 구하기
    * 시작 URL 집합: 크롤링 출발점. 가능한 많은 링크 탐색이 가능해야 함.
    * 미수집 URL 저장소: 크롤링 상태는 크게 `다운할`/`다운된` 으로 구분 가능. `다운할` 상태의 URL을 저장.
    * HTML 다운로더
    * 도메인 이름 변환기
    * 콘텐츠 파서: 다운로드 -> 파싱 & 검증 (여기에 시간이 오래 걸려서 크롤링 서버와 분리)
    * 중복 콘텐츠 필터 by 해시값 비교
    * 콘텐츠 저장소: 디스크, 메모리
    * URL 추출기: 상대 경로 -> 절대 경로
    * URL 필터: 미지원 확장자, 오류나는 경로 등을 크롤링 대상에서 배제
    * 이미 방문한 URL 필터: 블룸 필터/해시 테이블
    * URL 저장소: `다운된` 상태의 URL을 저장.
3. 상세 설계
    * DFS vs BFS
        - 크롤링 대상이 얼마나 깊은 depth를 가질지 모르기 때문에 DFS는 비추
        - BFS를 사용할 경우 같은 서버에 동시에 많은 요청을 보내게 됨 (impolite)
        - 순정 BFS는 URL간 우선 순위를 두지 않으나, PageRank/트래픽량/업데이트 빈도 등으로 처리 우선순위를 두는 것이 좋음
    * 미수집 URL 저장소 (의 확장)
        - 예의를 갖춘(polite) 크롤러
            + 동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청할 것
            + 호스트당 큐를 두고 그 큐를 순회하면서 작업 스레드에 할당하게 하기
        - URL 사이 우선순위, 신선도를 구별하는 크롤러
            + 유용성에 따라 우선순위를 나눌 수 있음 (ex. 페이지랭크, 트래픽 양, 갱신 빈도)
            + 순위당 큐를 두고 순위가 높을수록 선택할 확률이 높도록 설정
            + 신선도를 위해 주기적으로 재크롤링이 필요 (최적화 by 변경 이력, 우선 순위)
        - 전면에 우선순위를, 후면에 예의바른 동작을 보장하도록 설계할 수 있다
        - 수억 개의 데이터를 저장해야 해서 대부분 디스크에 저장하고 일부를 메모리의 큐에 두는 방식
    * HTML 다운로더
        - 웹사이트가 크롤러와 소통하는 표준 방법: Robots.txt
        - 성능 최적화 전략들
            + 분산 크롤링: 작업을 여러 서버에 분산
            + 도메인 이름 변환 결과 캐시 & cron job 등으로 주기적 갱신
            + 크롤링 서버를 크롤링 대상 서버와 지역적으로 가까이에 두기
            + 응답을 받기 위한 최대 시간을 미리 설정 (타임아웃)
    * 안정성 확보 전략
        - 안정 해시: 다운로더 서버를 쉽게 추가하고 삭제할 수 있게 함
        - 크롤링 상태 및 수집 데이터 저장: 장애 복구 쉽게
        - 예외 처리: 예외가 발생해도 시스템은 중단이 없어야 함
        - 데이터 검증
    * 확장성 확보 전략
        - 신규 기능을 플러그인 등으로 추가하여 전체 시스템 설계를 뒤엎지 않도록
    * 문제 있는 콘텐츠 감지 및 회피 전략
        - 중복 콘텐츠: 해시/체크섬 등으로 탐지
        - 거미 덫: 무한히 깊은 디렉터리 구조. URL 최대 길이를 제한하거나, 셀프 블랙리스트 작성
        - 데이터 노이즈: 광고, 스크립트, 스팸은 거르기
4. 마무리
    * 서버 사이드 렌더링
    * 페이지 필터링
    * 데이터베이스 다중화 및 샤딩
    * 수평적 규모 확장성 (stateless 서버)
    * 가용성, 일관성, 안정성
    * 데이터 분석

## 밑줄
- 저장소를 구현하는 데 쓰일 기술을 고를 때는 저장할 데이터의 유형, 크기, 저장소 접근 빈도, 데이터의 유효 기간 등을 종합적으로 고려해야 한다. (p.147)

## 질문
1. 왜 http를 다운로드할 때 dns를 따로 구현하는 쪽으로 만들었을까? 크롤링 과정에서는 일반적으로 ip 주소로 요청하나?
    * 외부 DNS에 요청하는 것보다 셀프 DNS를 쓰는 게 시간이 좀 더 짧게 걸려서인 것 같다.
    * 브라우저 단에서 자동으로 설정하는 부분이 있는데 그걸 구현한 것 같기도 하다.
2. 완전 중복인 데이터가 어쩌다 29%나 될까? 여기에는 단순 복붙 사이트도 포함될까?
    * 책 내 Reference 문서
        - [참고1 Study: 29% Of Sites Face Duplicate Content Issues & 80% Aren’t Using Schema.org Microdata](https://searchengineland.com/study-29-of-sites-face-duplicate-content-issues-80-arent-using-schema-org-microdata-232870)
        - [참고2 fingerprinting by random polynomials](http://www.xmailserver.org/rabin.pdf)
        - [참고3 Rabin fingerprint](https://en.wikipedia.org/wiki/Rabin_fingerprint)
3. 트래픽 양/변경 이력/갱신 빈도를 크롤러가 알 수 있는 방법은 뭘까?
    * 변경 이력이나 갱신 빈도는 크롤러 시스템이 따로 저장해두면 될 것 같기도 하다.
    * 크롤러가 타겟 서버의 트래픽 양을 파악하는 건 아니고 검색 엔진에서 유입되는 트래픽 양을 의미할 것 같다.
4. 만약 robots.txt가 금지하는 데이터를 크롤러가 크롤링 시도하면 어떻게 될까?
    * 단순 규약이라서 안 지킨다고 바로 패널티가 들어가진 않지만, 자체적으로 밴할 순 있을 듯
5. 크롤러가 서버 사이드 렌더링을 한다는 건 완성된 페이지를 만들기 위해 뭔가 시도한다는 의미일까? 어떻게 할까?
    * 다이나믹 크롤러를 의미하는 건가 싶음. 그 selenium 같은 걸로?