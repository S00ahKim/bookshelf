# 1. 사용자 수에 따른 규모 확장성

## 시스템 규모 확장을 위한 기법들
1. 웹 계층은 무상태 계층으로
    * 상태 정보가 웹서버로부터 물리적으로 분리되어 있는 구조는 안정적이고 단순하며 확장하기 쉽다.
2. 모든 계층에 다중화 도입
3. 가능한 한 많은 데이터 캐시할 것
    * 갱신은 적고 참조가 빈번할 경우 유용
    * 캐시 메모리는 과할당해서 성능이 떨어지지 않게 함
4. 여러 데이터센터를 지원할 것
    * 가장 가까운 데이터센터로 트래픽을 보낼 수 있도록 트래픽 우회 지원
    * 데이터를 여러 데이터센터에 걸쳐 다중화하면 어떤 센터의 장애가 요청 실패로 이어지지 않음
5. 정적 콘텐츠는 CDN을 통해 서빙할 것
6. 데이터 계층은 샤딩을 통해 그 규모를 확장할 것
    * 샤딩 = 대규모 DB를 샤드라고 부르는 작은 단위로 분할하는 기술
    * 모든 샤드는 같은 스키마를 쓰지만, 샤드에 보관되는 데이터 간에는 중복이 없다.
7. 각 계층은 독립적 서비스로 분할할 것
8. 시스템을 지속적으로 모니터링하고, 자동화 도구들을 활용할 것
    * 호스트 단위 메트릭: CPU, 메모리, 디스크 I/O
    * 종합 메트릭: DB 계층 성능, 캐시 계층 성능
    * 핵심 비즈니스 메트릭: 일별 능동 사용자(DAU), 수익, 재방문(retention)
    * 자동화? 지속적 통합, 빌드/테스트/배포 자동화


#### cf. noSQL
- 일반적으로 조인 연산 지원하지 않음
- 아주 낮은 응답 지연시간 요구
- 다루는 데이터가 비정형
- 아주 많은 양의 데이터를 저장해야 함


## 밑줄
- 사설 IP 주소는 같은 네트워크에 속한 서버 사이의 통신에만 쓰일 수 있는 IP 주소로, 인터넷을 통해서는 접속할 수 없다. 로드밸런서는 웹서버와 통신하기 위해 바로 이 사설 주소를 이용한다. (7p)
- 애플리케이션의 성능은 데이터베이스를 얼마나 자주 호출하느냐에 크게 좌우되는데 (11p)
- 메시지 큐는 메시지의 무손실(소비자가 꺼낼 때까지 안전히 보관)을 보장하는, 비동기 통신을 지원하는 컴포넌트다. (22p)
- 메시지 큐는 각 컴포넌트가 보다 느슨히 결합될 수 있도록 하고, 결함에 대한 내성을 높인다. (24p)


## 궁금한 것
- DB서버의 마스터 다운에 대비하기 위한 방식들, <다중 마스터>와 <원형 다중화>는 어떤 방식일까?
    * 다중 마스터? 말 그대로 마스터를 2개 (or 이상?) 두고 앱이 마스터를 선택해서 쓰기 작업 이후 해당 마스터가 다른 마스터에 복제하는 것 [참고](https://aws.amazon.com/ko/blogs/korea/building-highly-available-mysql-applications-using-amazon-aurora-mmsr/)
    * 원형 다중화? a.k.a. 순환 복제는 모든 master가 slave를 가질 때까지 각 서버를 마스터로 설정하고 다른 마스터의 슬레이브로 설정한다. 일종의 복제 체인을 돌고 돌아 흐름이 생성되는 개념. [참고](https://www.jamescoyle.net/how-to/1549-mysql-circular-replication)
- <동적 콘텐츠 캐싱>이란?
    * 요청 path, 쿼리스트링, 쿠키, 요청 헤더 등의 정보에 기반하여 html 페이지를 캐시하는 것 [참고](https://aws.amazon.com/ko/cloudfront/dynamic-content/)
- 상태 정보 의존적인 아키텍처가 실제 서비스에 서빙되는 경우가 있었을까...?
- [넷플릭스에서 데이터 센터 여럿에 어떻게 데이터를 다중화했을까?](https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b)
    * 상황: 트래픽이 너무 몰리거나 속도 문제가 생기면 다운되는 일 발생 & 한 region의 실패가 다른 쪽으로 이어짐
    * 선제조건
        + stateless한 서비스 (데이터 계층에서만 데이터와 상태 복제 발생)
        + 어떤 리소스든 한 region에서 접근 가능해야 함
        + region을 넘나드는 call은 없어야 함 & 데이터 복제가 비동기여야 함
    * 방법: 기본으로는 가까운 region에 라우팅 & 장애 발생시 다른 region으로 트래픽 보냄
    * 어려웠던 점들
        + 알맞은 region에 트래픽을 보내기 위한 효율적인 도구
            - UltraDNS(region간 이동)와 Route53(UltraDNS와 ELB 사이의 안정적이고 빠른 라우팅) 항목의 조합
            - Denominator: 여러 DNS 공급자를 제어하는 ​​단일 클라이언트 라이브러리와 커맨드 라인 제공
        + 엄청난 양의 이벤트를 처리하기 위한 traffic shaping 및 load shedding
            - traffic shaping?
                + 네트워크 통신량을 제어하기 위해 패킷을 지연시켜 대역폭 확보 & 통신 성능 최적화
            - load shedding? 
                + 네트워크 혼잡을 피하기 위해 새 패킷보다 오래된 패킷을 삭제하는 것
            - 솔루션은 zuul을 업그레이드해서 사용
                +  업그레이드한 기능: 잘못 라우팅된 요청을 식별하고 처리, 실패 트래픽 한계를 두어 자동으로 shed
        + region 간의 상태&데이터 비동기 복제
            - 아파치 카산드라: 고유 기능인 다방향 및 다중 데이터 센터(다중 지역) 비동기식 복제
            - 기존에도 운영은 했지만, write된 곳에서 대부분 소비됨 ([일관성 수준](https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlConfigConsistency.html): LOCAL_QUORUM, ONE)
            - 패러다임을 바꿔, 방금 작성한 이 region의 데이터에 대해 다른 region의 call에도 유효 응답이 내려오는지 확인함
            - 1ms 안의 응답을 보장하기 위해 memcached를 도입. region별 캐시 간의 일치성을 보장하기 위해 쓰기 작업이 있을 때마다 캐시를 무효화함 (by EvCache, SQS)
    * 배포 자동화: 여러 region에 대한 카나리 배포, 롤백 절차 존재 (로컬 배포를 하면서 전세계 배포가 되기 전에 문제 캐치함)
    * 시스템 검증을 위한 'monkey'들 도입
        + Chaos Gorilla: 한 region의 로컬 데이터센터가 다운되었을 때에도 서비스는 문제없나?
        + Split-brain: 일부 데이터 복제가 대기 중이어도 서비스는 문제없나?
        + Chaos Kong: 한 region 전체가 다운되었을 때에도 서비스는 문제없나? (비정상 region으로 라우팅된 사용자를 정상 region으로 보냄)


## 스터디에서
- [논의 이슈 링크](https://github.com/gogunbuntu-study/system-design-interview/issues/1)
- AWS의 CDN은 cloudfront다.
- <동적 콘텐츠 캐싱>이란?
    * 은행 서비스의 개인 서명 같은 경우 아예 html 페이지 전체를 캐싱하는 경우가 있다고 한다. 비용이 커서 자주는 아니라고 함.
    * 콘텐츠 외에 api 응답 같은 경우에도 캐시해서 아예 api 서버에 부하가 안 가게끔도 할 수 있다고 한다. [슬랙](https://www.youtube.com/watch?v=oVaTiRl9-v0&ab_channel=AmazonWebServices)의 예시.
- 상태 정보 의존적인 아키텍처가 실제 서비스에 서빙되는 경우가 있었을까...?
    * http의 성질이 stateless이므로 일반적인 경우는 아닐 것 같다.
    * 클라이언트-서버 간 신뢰관계가 확실하다는 일부 케이스에서 사용 가능할 것 같다.
    * 금융상품 가입 같은 과정이 복잡하고 긴 경우 + 캐싱하기 어려울 경우에 쓸 수 있을 것 같다.
    * 상태 정보 의존적인 아키텍처를 [스티키 세션](https://smjeon.dev/web/sticky-session/)이라고 한다. 그런데 여기에는 단점들이 있기 때문에(과부하 등) 세션 서버를 분리하는 세션 클러스터링 방식을 도입한다.
    * 로컬 캐시 히트가 높아질 수 있을 것 같다.
- 클라이언트 캐시 전략
    * [서비스 워커 캐싱](https://web.dev/i18n/ko/service-worker-caching-and-http-caching/)
        + 서비스 워커 : 브라우저가 백그라운드에서 실행하는 스크립트. 웹페이지와는 별개로 작동하며 캐시와 상호작용, 푸쉬 알림, 백그라운드 동기화 등의 기능을 제공 ([예시](https://googlechrome.github.io/samples/service-worker/selective-caching/))
        + 모든 브라우저가 지원하는 것은 아니다.
        + [서버가 아닌 클라이언트단에서 캐시 컨트롤 가능](https://developer.chrome.com/docs/workbox/service-worker-overview/#access-to-a-javascript-driven-caching-api)
        + 이 개념은 pwa 라고 앱처럼 동작하는 웹을 만들려고 등장했다고 한다.
    * [memory cache, disk cache](https://mygumi.tistory.com/275)
    * stale-while-revalidate
        + swr개념을 이용한 server state 라이브러리에서의 in memory 캐싱(react-query, rtx-query, swr, apollo client, ..) [react-query 예제](https://react-query.tanstack.com/guides/caching#_top)
- L4 로드밸런서 vs L7 로드밸런서
    * L4는 안전하고 빠르다.
    * L7은 섬세한 라우팅을 지원하고 캐시할 수 있다.
- spring boot의 기본 캐시 의존성(spring-boot-starter-cache)을 추가했을때는 어떤것이 기본으로 설정될까? [참고](https://stalker5217.netlify.app/spring/cache/)
    * spring의 캐시 인터페이스 'Cache Manager' (스프링 3부터 제공)
    * @EnableCaching(캐시하겠다), @Cacheable(이걸 캐시해라), @CacheEvict(캐시를 지워라)
    * 캐시 추상화에서 사용할 스토리지로 ConcurrentMap 기반의 SimpleCacheManager 생성
    * Cache Manager의 구현체(ex. EhCache, Redis, Caffeine Cache, JCache)를 빈으로 등록하면 다른 스토리지 사용 가능