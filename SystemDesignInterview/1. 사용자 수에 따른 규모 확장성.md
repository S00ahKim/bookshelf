# 1. 사용자 수에 따른 규모 확장성

## 시스템 규모 확장을 위한 기법들
1. 웹 계층은 무상태 계층으로
    * 상태 정보가 웹서버로부터 물리적으로 분리되어 있는 구조는 안정적이고 단순하며 확장하기 쉽다.
2. 모든 계층에 다중화 도입
3. 가능한 한 많은 데이터 캐시할 것
    * 갱신은 적고 참조가 빈번할 경우 유용
    * 캐시 메모리는 과할당해서 성능이 떨어지지 않게 함
4. 여러 데이터센터를 지원할 것
    * 가장 가까운 데이터센터로 트래픽을 보낼 수 있도록 트래픽 우회 지원
    * 데이터를 여러 데이터센터에 걸쳐 다중화하면 어떤 센터의 장애가 요청 실패로 이어지지 않음
5. 정적 콘텐츠는 CDN을 통해 서빙할 것
6. 데이터 계층은 샤딩을 통해 그 규모를 확장할 것
    * 샤딩 = 대규모 DB를 샤드라고 부르는 작은 단위로 분할하는 기술
    * 모든 샤드는 같은 스키마를 쓰지만, 샤드에 보관되는 데이터 간에는 중복이 없다.
7. 각 계층은 독립적 서비스로 분할할 것
8. 시스템을 지속적으로 모니터링하고, 자동화 도구들을 활용할 것
    * 호스트 단위 메트릭: CPU, 메모리, 디스크 I/O
    * 종합 메트릭: DB 계층 성능, 캐시 계층 성능
    * 핵심 비즈니스 메트릭: 일별 능동 사용자(DAU), 수익, 재방문(retention)
    * 자동화? 지속적 통합, 빌드/테스트/배포 자동화


#### cf. noSQL
- 일반적으로 조인 연산 지원하지 않음
- 아주 낮은 응답 지연시간 요구
- 다루는 데이터가 비정형
- 아주 많은 양의 데이터를 저장해야 함


## 밑줄
- 사설 IP 주소는 같은 네트워크에 속한 서버 사이의 통신에만 쓰일 수 있는 IP 주소로, 인터넷을 통해서는 접속할 수 없다. 로드밸런서는 웹서버와 통신하기 위해 바로 이 사설 주소를 이용한다. (7p)
- 애플리케이션의 성능은 데이터베이스를 얼마나 자주 호출하느냐에 크게 좌우되는데 (11p)
- 메시지 큐는 메시지의 무손실(소비자가 꺼낼 때까지 안전히 보관)을 보장하는, 비동기 통신을 지원하는 컴포넌트다. (22p)
- 메시지 큐는 각 컴포넌트가 보다 느슨히 결합될 수 있도록 하고, 결함에 대한 내성을 높인다. (24p)


## 궁금한 것
- DB서버의 마스터 다운에 대비하기 위한 방식들, <다중 마스터>와 <원형 다중화>는 어떤 방식일까?
    * 다중 마스터? 말 그대로 마스터를 2개 (or 이상?) 두고 앱이 마스터를 선택해서 쓰기 작업 이후 해당 마스터가 다른 마스터에 복제하는 것 [참고](https://aws.amazon.com/ko/blogs/korea/building-highly-available-mysql-applications-using-amazon-aurora-mmsr/)
    * 원형 다중화? a.k.a. 순환 복제는 모든 master가 slave를 가질 때까지 각 서버를 마스터로 설정하고 다른 마스터의 슬레이브로 설정한다. 일종의 복제 체인을 돌고 돌아 흐름이 생성되는 개념. [참고](https://www.jamescoyle.net/how-to/1549-mysql-circular-replication)
- <동적 콘텐츠 캐싱>이란?
    * 요청 path, 쿼리스트링, 쿠키, 요청 헤더 등의 정보에 기반하여 html 페이지를 캐시하는 것 [참고](https://aws.amazon.com/ko/cloudfront/dynamic-content/)
- 상태 정보 의존적인 아키텍처가 실제 서비스에 서빙되는 경우가 있었을까...?
- [넷플릭스에서 데이터 센터 여럿에 어떻게 데이터를 다중화했을까?](https://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b)
    * 상황: 트래픽이 너무 몰리거나 속도 문제가 생기면 다운되는 일 발생 & 한 region의 실패가 다른 쪽으로 이어짐
    * 선제조건
        + stateless한 서비스 (데이터 계층에서만 데이터와 상태 복제 발생)
        + 어떤 리소스든 한 region에서 접근 가능해야 함
        + region을 넘나드는 call은 없어야 함 & 데이터 복제가 비동기여야 함
    * 방법: 기본으로는 가까운 region에 라우팅 & 장애 발생시 다른 region으로 트래픽 보냄
    * 어려웠던 점들
        + 알맞은 region에 트래픽을 보내기 위한 효율적인 도구
            - UltraDNS(region간 이동)와 Route53(UltraDNS와 ELB 사이의 안정적이고 빠른 라우팅) 항목의 조합
            - Denominator: 여러 DNS 공급자를 제어하는 ​​단일 클라이언트 라이브러리와 커맨드 라인 제공
        + 엄청난 양의 이벤트를 처리하기 위한 traffic shaping 및 load shedding
            - traffic shaping?
                + 네트워크 통신량을 제어하기 위해 패킷을 지연시켜 대역폭 확보 & 통신 성능 최적화
            - load shedding? 
                + 네트워크 혼잡을 피하기 위해 새 패킷보다 오래된 패킷을 삭제하는 것
            - 솔루션은 zuul을 업그레이드해서 사용
                +  업그레이드한 기능: 잘못 라우팅된 요청을 식별하고 처리, 실패 트래픽 한계를 두어 자동으로 shed
        + region 간의 상태&데이터 비동기 복제
            - 아파치 카산드라: 고유 기능인 다방향 및 다중 데이터 센터(다중 지역) 비동기식 복제
            - 기존에도 운영은 했지만, write된 곳에서 대부분 소비됨 ([일관성 수준](https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/dml/dmlConfigConsistency.html): LOCAL_QUORUM, ONE)
            - 패러다임을 바꿔, 방금 작성한 이 region의 데이터에 대해 다른 region의 call에도 유효 응답이 내려오는지 확인함
            - 1ms 안의 응답을 보장하기 위해 memcached를 도입. region별 캐시 간의 일치성을 보장하기 위해 쓰기 작업이 있을 때마다 캐시를 무효화함 (by EvCache, SQS)
    * 배포 자동화: 여러 region에 대한 카나리 배포, 롤백 절차 존재 (로컬 배포를 하면서 전세계 배포가 되기 전에 문제 캐치함)
    * 시스템 검증을 위한 'monkey'들 도입
        + Chaos Gorilla: 한 region의 로컬 데이터센터가 다운되었을 때에도 서비스는 문제없나?
        + Split-brain: 일부 데이터 복제가 대기 중이어도 서비스는 문제없나?
        + Chaos Kong: 한 region 전체가 다운되었을 때에도 서비스는 문제없나? (비정상 region으로 라우팅된 사용자를 정상 region으로 보냄)